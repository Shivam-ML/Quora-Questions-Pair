{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM\n",
    "import keras.layers as lyr\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Bidirectional\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'questions.csv'\n",
    "#TRAIN_CSV = 'sample.csv'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "def stringToWordList(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\(\", \" ( \", text)\n",
    "    text = re.sub(r\"\\)\", \" ) \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "vocab = dict()\n",
    "maxSeqLength = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    for question in questions_cols:\n",
    "        q2num = []\n",
    "        for word in stringToWordList(row[question]):\n",
    "            if word in stops and word not in word2vec.vocab:\n",
    "                continue\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)+1\n",
    "            q2num.append(vocab[word])\n",
    "        train_df.set_value(index,question,q2num)\n",
    "        if len(q2num) > maxSeqLength:\n",
    "            maxSeqLength = len(q2num)\n",
    "\n",
    "embeddingDim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocab) + 1, embeddingDim)\n",
    "embeddings[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 3, 4, 5, 4, 6, 7, 8, 9, 10, 8, 11]</td>\n",
       "      <td>[1, 2, 3, 4, 5, 4, 6, 7, 8, 9, 10]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2, 3, 12, 13, 14, 15, 16, 15, 17, 18]</td>\n",
       "      <td>[1, 19, 20, 21, 3, 22, 23, 24, 3, 13, 14, 15, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>[26, 27, 16, 28, 3, 29, 30, 31, 32, 33, 34, 35]</td>\n",
       "      <td>[26, 27, 31, 29, 36, 37, 5, 38, 39, 40]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>[41, 42, 16, 43, 44, 45, 26, 27, 16, 46, 47]</td>\n",
       "      <td>[48, 3, 49, 50, 51, 52, 53, 54, 51, 2, 55, 5, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[56, 57, 58, 8, 59, 60, 61, 62, 63, 64, 65, 66]</td>\n",
       "      <td>[56, 67, 19, 68, 8, 62, 59]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  qid1  qid2  \\\n",
       "0           0   0     1     2   \n",
       "1           1   1     3     4   \n",
       "2           2   2     5     6   \n",
       "3           3   3     7     8   \n",
       "4           4   4     9    10   \n",
       "\n",
       "                                         question1  \\\n",
       "0        [1, 2, 3, 4, 5, 4, 6, 7, 8, 9, 10, 8, 11]   \n",
       "1        [1, 2, 3, 12, 13, 14, 15, 16, 15, 17, 18]   \n",
       "2  [26, 27, 16, 28, 3, 29, 30, 31, 32, 33, 34, 35]   \n",
       "3     [41, 42, 16, 43, 44, 45, 26, 27, 16, 46, 47]   \n",
       "4  [56, 57, 58, 8, 59, 60, 61, 62, 63, 64, 65, 66]   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0                 [1, 2, 3, 4, 5, 4, 6, 7, 8, 9, 10]             0  \n",
       "1  [1, 19, 20, 21, 3, 22, 23, 24, 3, 13, 14, 15, ...             0  \n",
       "2            [26, 27, 31, 29, 36, 37, 5, 38, 39, 40]             0  \n",
       "3  [48, 3, 49, 50, 51, 52, 53, 54, 51, 2, 55, 5, ...             0  \n",
       "4                        [56, 67, 19, 68, 8, 62, 59]             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 20000\n",
    "validation_size = 40000\n",
    "training_size = len(train_df) - validation_size\n",
    "\n",
    "X = train_df[questions_cols]\n",
    "Y = train_df['is_duplicate']\n",
    "\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train_val, Y_train_val, test_size=validation_size)\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_test = {'left': X_test.question1, 'right': X_test.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_validation, X_test], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=maxSeqLength)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1009 12:32:55.383420 79092 deprecation_wrapper.py:119] From C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1009 12:32:56.883743 79092 deprecation_wrapper.py:119] From C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1009 12:32:58.023118 79092 deprecation_wrapper.py:119] From C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1009 12:32:58.710837 79092 deprecation_wrapper.py:119] From C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1009 12:32:58.712822 79092 deprecation_wrapper.py:119] From C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model variables\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 1\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(maxSeqLength,), dtype='int32')\n",
    "right_input = Input(shape=(maxSeqLength,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings), embeddingDim, weights=[embeddings], input_length=maxSeqLength, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = Bidirectional(LSTM(n_hidden))\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1009 12:33:08.160382 79092 deprecation.py:506] From C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  \n",
      "W1009 12:33:08.276046 79092 deprecation_wrapper.py:119] From C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the embeddings with their product and squared difference.\n",
    "p = lyr.multiply([left_output, right_output])\n",
    "negative_right_output = lyr.Lambda(lambda x: -x)(right_output)\n",
    "d = lyr.add([left_output, negative_right_output])\n",
    "q = lyr.multiply([d, d])\n",
    "v = [left_output, right_output, p, q]\n",
    "lstm_output = lyr.concatenate(v)\n",
    "\n",
    "merged = lyr.BatchNormalization()(lstm_output)\n",
    "merged = lyr.Dense(64, activation='relu')(merged)\n",
    "merged = lyr.Dropout(0.2)(merged)\n",
    "merged = lyr.BatchNormalization()(merged)\n",
    "preds = lyr.Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(input=[left_input,right_input], output=preds)\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "model.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1009 12:33:20.974612 79092 deprecation.py:323] From C:\\Users\\20131073\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 344351 samples, validate on 40000 samples\n",
      "Epoch 1/1\n",
      "344351/344351 [==============================] - 2787s 8ms/step - loss: 0.1615 - acc: 0.7647 - val_loss: 0.1422 - val_acc: 0.7956\n",
      "Training time finished.\n",
      "1 epochs in 0:46:29.620206\n"
     ]
    }
   ],
   "source": [
    "training_start_time = time()\n",
    "\n",
    "lstm_trained = model.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size,epochs=n_epoch,\n",
    "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###https://github.com/Dhawnit/Duplicate-Question-Pair-Detection-using-Deep-Learning/blob/master/nlpProj.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 79.55%\n"
     ]
    }
   ],
   "source": [
    "scores =model.evaluate([X_test['left'], X_test['right']], Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 5658, 8721, 2496],\n",
       "       [   0,    0,    0, ...,  258, 4470,  518],\n",
       "       [   0,    0,    0, ...,   84,  212,  781],\n",
       "       ...,\n",
       "       [   0,    0,    0, ..., 1267,    8, 9667],\n",
       "       [   0,    0,    0, ...,    2,   83, 7840],\n",
       "       [   0,    0,    0, ...,  191,   47, 3570]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
